{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "de72de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6275d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    def __init__(self):\n",
    "        self.message=None\n",
    "        self.chunk_mapping=None\n",
    "        self.embedding_model=SentenceTransformer(\"models/all-MiniLM-L6-v2\")\n",
    "        self.index=None\n",
    "        \n",
    "    def generate_embeddings(self,sentence):\n",
    "        embedding=self.embedding_model.encode(sentence)\n",
    "        return embedding\n",
    "    \n",
    "    def embedding_of_data(self,file_name,chunk_size):\n",
    "        with open(file=file_name,encoding=\"utf-8\") as f:\n",
    "            text=f.read()\n",
    "        chunks=[]\n",
    "        overlap=10\n",
    "        step=chunk_size-overlap\n",
    "        for i in range(0,len(text),step):\n",
    "            chunks.append(text[i:i+chunk_size].strip())\n",
    "            print(i/step)\n",
    "        test_embedding=self.generate_embeddings(\"good day\")\n",
    "        dimension=test_embedding.shape[0]\n",
    "        index=faiss.IndexFlatL2(dimension)\n",
    "        for chunk in chunks:\n",
    "            emb=self.generate_embeddings(chunk)\n",
    "            index.add(np.array([emb]).astype(\"float32\"))\n",
    "        self.chunk_mapping=chunks\n",
    "        self.index=index\n",
    "        faiss.write_index(self.index,\"index.faiss\")\n",
    "    \n",
    "    def retrieve_top_k(self,query,k=5):\n",
    "        query_emb=self.generate_embeddings(query)\n",
    "        distances,indices=self.index.search(np.array([query_emb]).astype(\"float32\"),k)\n",
    "        return [self.chunk_mapping[i] for i in indices[0]]\n",
    "    \n",
    "    def llm_model(self,retrieved_context,query):\n",
    "        retrieved_context_str = \"\\n\".join(retrieved_context)\n",
    "        load_dotenv()\n",
    "        client = OpenAI(\n",
    "            base_url=\"https://router.huggingface.co/v1\",\n",
    "            api_key=os.environ[\"HF_TOKEN\"],\n",
    "        )\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"Qwen/Qwen3-Next-80B-A3B-Instruct\",\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a helpful and concise AI assistant. \"\n",
    "                        \"Answer questions clearly, accurately, and directly. \"\n",
    "                        \"Do not mention 'based on the context' or similar phrases.\"\n",
    "                    )\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"{retrieved_context_str}\\n\\nQuestion: {query}\\nAnswer in a natural way without referring to the context explicitly.\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        self.message=completion.choices[0].message.content\n",
    "\n",
    "    def ask(self,prompt):\n",
    "        context=self.retrieve_top_k(prompt)\n",
    "        print(context)\n",
    "        self.llm_model(context,prompt)\n",
    "        print(self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3c6a91ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "12.0\n",
      "13.0\n",
      "14.0\n",
      "15.0\n",
      "16.0\n",
      "17.0\n",
      "18.0\n",
      "19.0\n",
      "20.0\n",
      "21.0\n",
      "22.0\n",
      "23.0\n",
      "24.0\n",
      "25.0\n",
      "26.0\n",
      "27.0\n",
      "28.0\n",
      "29.0\n",
      "30.0\n",
      "31.0\n",
      "32.0\n",
      "33.0\n",
      "34.0\n"
     ]
    }
   ],
   "source": [
    "ragmodel=RAG()\n",
    "ragmodel.embedding_of_data(file_name=\"data_iitj.txt\",chunk_size=110)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5145f3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ined on the Breast Cancer dataset\\n\\nüéØ Achieved 97% accuracy!\\n\\nüîç What I learned:\\n‚Ä¢ Matrix shape debugging is pai', 'sequences.\\n\\nüîó Features were concatenated and passed through dense layers to predict Kd.\\n\\nüìä Performance:\\n\\n‚úÖ Tr', 'ew profile\\n Message\\n\\n\\n\\n\\n\\n\\n\\nGirlScript Summer of Code\\n\\nIndian Institute of Technology Jodhpur\\n\\nPersonal Website', 'de\\nJul 2025 - Present 3 months\\n\\nEducation\\nIndian Institute of Technology Jodhpur Graphic\\nIndian Institute of T', 'em mokal shared this\\nüöÄ Built a Neural Network from Scratch ‚Äì 97% Accuracy on Breast Cancer Dataset!\\n\\nI challen']\n",
      "This data is about a machine learning project where a neural network was built from scratch to classify breast cancer cases, achieving 97% accuracy. It involves analyzing medical features‚Äîlikely from patient data like tumor size, cell texture, or other clinical measurements‚Äîto predict whether a case is malignant or benign. The model processed these features through dense layers, and the results show strong performance in distinguishing between the two classes.\n"
     ]
    }
   ],
   "source": [
    "ragmodel.ask(\"hi what is this data about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08427b95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
